## Moral Machine — Dilemas Éticos em Veículos Autônomos
*2016–2018 — MIT Media Lab*

---

**Resumo do Caso**

O projeto *Moral Machine* coletou milhões de respostas de pessoas ao redor do mundo sobre dilemas morais envolvendo veículos autônomos: por exemplo, salvar passageiros ou pedestres, priorizar jovens ou idosos, humanos ou animais.

O estudo revelou que não existe consenso ético global sobre essas decisões. As preferências variavam significativamente entre culturas, regiões e contextos socioeconômicos.

---

**Qual o risco ético envolvido?**

Automatização do juízo moral e normalização de decisões éticas irreversíveis baseadas em estatísticas.

---

**Consequências observadas**

* Tornou visível que sistemas autônomos não “decidem” eticamente — eles **executam valores embutidos** por desenvolvedores e instituições.
* Abriu debates internacionais sobre quem deve definir os critérios morais de máquinas que impactam vidas humanas.
* Evidenciou que escolhas algorítmicas podem cristalizar vieses culturais como se fossem universais.

---

**Relação com os Atos de Custódia**

Relaciona-se diretamente ao:

* **Ato de Custódia 05 — Usos Distorcidos** (delegação de decisões morais)
* **Campo de Riscos — Automatização do Juízo Ético**

Este caso ilustra com clareza que, quando o juízo humano é substituído por regras estatísticas, a responsabilidade se torna difusa — e a ética, programável.

