## Gender Shades — Viés em Sistemas de Reconhecimento Facial

*2018 – MIT Media Lab / Algorithmic Justice League*

**Resumo do Caso**

O estudo “Gender Shades”, conduzido por Joy Buolamwini, analisou sistemas comerciais de reconhecimento facial usados por grandes empresas de tecnologia. Os resultados mostraram taxas de erro extremamente mais altas para mulheres negras do que para homens brancos.

**Qual o risco ético envolvido?**

Viés algorítmico e reprodução estrutural de desigualdades sociais.

**Consequências observadas**

Empresas como IBM e Microsoft foram pressionadas a revisar políticas, suspender vendas e repensar o uso da tecnologia por forças policiais.

**Relação com os Atos de Custódia**

Relaciona-se diretamente ao **Ato 05 — Usos Distorcidos** e ao campo de risco de **Justiça e Discriminação Algorítmica**.

