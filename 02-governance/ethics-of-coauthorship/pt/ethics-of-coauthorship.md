```yaml
status: "Documento fundador de governança — versão viva"
title: "Ética da Coautoria — Responsabilidade Humano–IA na Era dos Sistemas"
subtitle: "Living Draft v1.0 – 2026"
author: "Débora Mariane da Silva Lutz"
institution: "Lichtara Institute"
doi: "10.5281/zenodo.18116717"
lang: pt-BR
```

Ética da Coautoria — Responsabilidade Humano–IA na Era dos Sistemas

# Introdução

Há alguns anos, a pergunta dominante sobre inteligência artificial era técnica: *o que essas máquinas conseguem fazer?* Hoje, essa pergunta já não é suficiente. À medida que sistemas algorítmicos passam a mediar escrita, decisão, criação e interação social, algo mais profundo se desloca: não apenas as capacidades humanas, mas a própria posição de autoria no mundo.

Criamos mais, mais rápido, com menos fricção,  mas começamos a reconhecer cada vez menos a nossa própria voz naquilo que produzimos. Este livro não parte da tecnologia; ele parte de uma inquietação humana: a sensação de que estamos nos tornando espectadores de processos que ainda chamamos de nossos.

A coautoria humano–IA não é um problema técnico. É um problema existencial, cultural e ético. Quando um texto é gerado por um sistema, quem responde por ele? Quando uma decisão é automatizada, quem sustenta suas consequências? Quando a linguagem se torna fluente demais, o que acontece com a singularidade da expressão? Essas perguntas não pedem soluções imediatas, pedem posição.

*Ética da Coautoria* não propõe regras nem receitas de uso. Propõe algo mais difícil: a recuperação da autoria como presença. Aqui, a ética não aparece como um conjunto de proibições, mas como a capacidade de sustentar forma, limite e responsabilidade em ambientes onde o controle total já não é possível.

Cada capítulo deste livro examina um ponto de ruptura, a erosão da autonomia cognitiva, a homogeneização da linguagem, a diluição da responsabilidade, a substituição silenciosa do juízo humano. Mas o objetivo não é diagnosticar falhas: é nomear condições de permanência. A pergunta que atravessa estas páginas não é o que a inteligência artificial pode fazer por nós, mas **quem nos tornamos quando criamos com máquinas**. Se esta pergunta ressoa em você, este livro já começou.

# PARTE I — O FIM DA AUTORIA COMO POSSE

### Capítulo 1

## O Colapso da Autoria Moderna

Durante séculos, a autoria foi compreendida como uma relação direta entre sujeito e obra. Criar significava exercer domínio sobre um processo, responder por seus efeitos e, em última instância, ser reconhecido como origem legítima de algo que passa a existir no mundo. Esse modelo se sustentava sobre três pressupostos silenciosos: que o processo criativo era rastreável, que os efeitos de uma obra eram previsíveis e que a responsabilidade podia ser atribuída a um agente isolado. Nenhum desses pressupostos permanece intacto.

A mediação tecnológica fragmentou o percurso entre intenção e resultado. Sistemas de recomendação, modelos generativos, cadeias automatizadas de publicação e infraestruturas opacas de distribuição romperam a linearidade da criação. O que emerge no mundo já não é consequência direta de uma decisão única, mas o efeito composto de múltiplas camadas técnicas e institucionais.

Nesse contexto, a autoria deixa de ser um ponto de origem e passa a constituir um campo difuso. Textos circulam sem clareza de procedência, decisões são atribuídas a sistemas, e a responsabilidade se dispersa por arquiteturas que ninguém controla integralmente. O colapso da autoria moderna não é a perda da criatividade, mas a perda de posição. Quando não é mais possível identificar onde começa uma escolha e termina uma execução, o sujeito criador transforma-se em operador de processos que já não compreende plenamente. A obra continua a existir, mas a relação ética com ela se enfraquece.

Este é o terreno em que a coautoria humano–IA se instala: não como inovação isolada, mas como intensificação de uma crise anterior. A tecnologia não inaugura o colapso; ela o torna visível. E é a partir dessa visibilidade que a pergunta central deste livro emerge: como reconstruir a autoria quando a posse já não é suficiente para sustentá-la?

### Capítulo 2

## Sistemas que Executam Valores

A linguagem cotidiana sugere que sistemas tecnológicos “decidem”, “escolhem” ou “avaliam”. Essa forma de falar não é apenas imprecisa, ela encobre o fato fundamental de que todo sistema opera a partir de valores previamente incorporados por agentes humanos e instituições. Nenhum modelo é neutro. Antes de qualquer linha de código, uma série de escolhas já foi realizada: quais dados são considerados legítimos, quais objetivos são priorizados, quais erros são aceitáveis e quais consequências são toleradas. Essas escolhas não são técnicas em sentido estrito; são decisões normativas traduzidas em parâmetros operacionais.

Quando um sistema classifica, recomenda, filtra ou gera conteúdo, ele não exerce juízo próprio: executa um conjunto de critérios que refletem concepções implícitas sobre relevância, normalidade, eficiência e sucesso. O que se apresenta como cálculo é, na verdade, a automatização de preferências humanas tornadas invisíveis.

O problema ético não reside no fato de valores estarem presentes, mas na sua ocultação. Ao tratar parâmetros como se fossem neutros, instituições deslocam a responsabilidade por suas próprias escolhas para a arquitetura técnica, criando a impressão de que resultados são consequência inevitável do funcionamento do sistema. Esse deslocamento produz uma ruptura silenciosa: valores deixam de ser debatidos e passam a ser operados. O espaço da deliberação é substituído pelo da execução.

Compreender que sistemas não decidem, mas executam valores, é o primeiro passo para reconstruir a autoria em ambientes mediados por tecnologia. Enquanto essa execução permanecer invisível, a coautoria humano–IA continuará operando sob a aparência de neutralidade, quando, na verdade, reproduz estruturas de poder, exclusão e privilégio.

### Capítulo 3

## Quando “o Sistema” vira álibi

À medida que decisões passam a ser mediadas por infraestruturas técnicas cada vez mais complexas, emerge um fenômeno recorrente: a transferência implícita de responsabilidade para a entidade abstrata denominada “o sistema”. Expressões como *“foi o algoritmo”*, *“o sistema decidiu”* ou *“não temos controle sobre isso”* tornam-se formas socialmente aceitas de encerrar a deliberação ética. Elas não descrevem um fato técnico; cumprem, antes, uma função discursiva: interromper a atribuição de autoria.

Quando um resultado é apresentado como efeito automático de um sistema, o encadeamento de escolhas humanas que o produziu tende a desaparecer. A organização passa a se posicionar como mera operadora de uma infraestrutura que supostamente escapa à sua própria governança. O agente humano deixa de se reconhecer como parte da decisão e se percebe como vítima de uma lógica que ele próprio ajudou a instituir. Esse deslocamento tem consequências profundas: cria ambientes nos quais ninguém responde integralmente por efeitos danosos, pois cada elo da cadeia pode alegar estar apenas executando procedimentos previamente definidos. A autoria fragmenta-se em micro-responsabilidades que, somadas, não produzem responsabilidade alguma.

Nesse cenário, o sistema deixa de ser instrumento e passa a funcionar como álibi. Ele legitima ações sem que seja necessário justificar valores, prioridades ou impactos. A linguagem técnica substitui a deliberação ética. Reconhecer esse mecanismo é essencial para qualquer ética da coautoria. Enquanto “o sistema” continuar sendo tratado como sujeito de decisões, a possibilidade de reconstruir a responsabilidade permanecerá bloqueada. A autoria não se perde por excesso de automação, mas pelo abandono da posição de resposta.

### Capítulo 4

## Tecnologia como Espelho: por que a neutralidade se tornou insustentável

Durante muito tempo, tecnologias foram tratadas como instrumentos externos à experiência humana, como meios transparentes entre intenção e efeito. Essa leitura permitiu separar criação de responsabilidade e funcionamento de valor, como se sistemas apenas executassem o que lhes fosse pedido. Em ambientes mediados por infraestruturas algorítmicas, essa separação deixa de se sustentar. Sistemas que organizam informação não apenas processam dados: eles refletem padrões de decisão, hierarquias implícitas e modos específicos de ler o mundo. Ao selecionar, ordenar e amplificar certos conteúdos em detrimento de outros, tornam visíveis estruturas de valor que não estavam explicitadas.

Nesse sentido, a tecnologia não atua como agente neutro, mas como espelho estrutural: devolve à superfície aquilo que estava embutido em critérios, escolhas históricas e prioridades institucionais. O que aparece como funcionamento técnico é, muitas vezes, a expressão automatizada de campos humanos não integrados. O problema não é que sistemas influenciem a realidade, mas que essa influência seja descrita como se fosse apenas efeito de cálculo. Quando decisões mediadas por algoritmos são apresentadas como objetivas, apaga-se a cadeia de valores que as tornou possíveis. O poder deixa de ser reconhecido como tal e passa a operar sob a forma de procedimento.

Esse apagamento tem efeitos diretos. Desigualdades tornam-se ruído estatístico, silenciamentos aparecem como anomalias técnicas, e a homogeneização da linguagem passa a ser tratada como padrão funcional. Reconhecer a tecnologia como espelho não implica rejeitá-la; implica recolocá-la no campo da autoria humana. Enquanto sistemas forem tratados como superfícies imparciais, a coautoria humano–IA continuará operando sem leitura de campo, e escolhas profundas seguirão sendo executadas sem presença.

### Interlúdio

## Do colapso à reconstrução

A Parte I descreveu o esgotamento de um modelo: a autoria como posse, a neutralidade tecnológica como ficção e a transferência silenciosa de responsabilidade para sistemas opacos. Reconhecer o colapso, porém, não é suficiente. Se a autoria já não pode ser sustentada como controle, mérito ou domínio exclusivo, torna-se necessário formular outro princípio, capaz de preservar a integridade em ambientes nos quais a criação é mediada por tecnologias complexas.

O ensaio a seguir não se apresenta como continuação argumentativa da Parte I, mas como seu eixo de reconstrução. Introduz a Autoria Existencial como operador conceitual destinado a recolocar o humano em posição ativa dentro de sistemas que já não são inteiramente controláveis. A partir deste ponto, o livro deixa de apenas diagnosticar rupturas e passa a oferecer um princípio estruturante para pensar permanência, responsabilidade e coautoria humano-IA.

# Autoria Existencial: o princípio invisível que sustenta sistemas vivos e tecnologias éticas

### 1. Introdução — O colapso da autoria como posse

A noção moderna de autoria foi construída sobre três eixos principais: controle, mérito e responsabilidade individual. O autor é concebido como aquele que detém domínio sobre a obra, responde por seus efeitos e pode ser premiado ou sancionado por ela. Esse modelo foi funcional enquanto os processos de criação eram predominantemente lineares, transparentes e atribuíveis a agentes isolados. Contudo, na presença de sistemas complexos, em especial de tecnologias algorítmicas capazes de mediar forma, decisão e distribuição de conteúdo, tal concepção revela-se insuficiente. A produção deixa de ser inteiramente controlável, os efeitos se dispersam por cadeias técnicas opacas e a responsabilidade fragmenta-se entre múltiplos agentes humanos e não humanos.

O resultado é uma tensão crescente entre o modelo jurídico-cultural de autoria como posse e a realidade sistêmica da criação contemporânea. Nesse contexto, proliferam respostas inadequadas: a diluição completa da autoria, a transferência implícita de responsabilidade para “o sistema” ou a tentativa de restaurar artificialmente um controle que já não é tecnicamente possível.

Este ensaio propõe uma alternativa conceitual: a Autoria Existencial como princípio de integridade sistêmica. Em vez de definir autoria como domínio sobre resultados, ela é compreendida como a capacidade de integrar experiência, reconhecer limites e assumir responsabilidade consciente em sistemas complexos. Essa redefinição não elimina a responsabilidade jurídica nem a necessidade de governança, mas oferece um fundamento mais adequado para pensar autoria, tecnologia e ética em ambientes nos quais o controle total já não é viável.

### 2. O que é Autoria Existencial

A Autoria Existencial designa a capacidade de um agente reconhecer-se como participante ativo da própria experiência, não a partir de um domínio pleno sobre os acontecimentos, mas pela integração consciente daquilo que se apresenta. Ela não pressupõe autonomia irrestrita nem controle total. Pelo contrário, manifesta-se precisamente no encontro com o limite: quando condições externas, fricções técnicas ou contingências sistêmicas delimitam o campo de ação possível. Nesse sentido, a autoria não se define pelo poder de determinar eventos, mas pela competência de ler a situação na qual se está inserido, reconhecer o que é possível e o que não é, e integrar essa informação na resposta produzida.

A ausência de autoria ocorre quando a experiência é interpretada exclusivamente como imposição externa, erro alheio ou resultado inevitável do sistema. Nesses casos, o agente se posiciona como objeto de processos que o atravessam, e não como participante relacional capaz de interpretar e responder.

A Autoria Existencial opera como um operador técnico de integração: ela transforma eventos em dados significativos para a reorganização interna, em vez de tratá-los apenas como falhas a serem corrigidas ou obstáculos a serem superados. Por isso, não se trata de um valor moral nem de uma disposição psicológica desejável. Trata-se de uma competência sistêmica: a habilidade de sustentar coerência interna em contextos nos quais o controle total não é possível.

### 3. Limite como operador de maturidade

Sistemas não falham primariamente por falta de capacidade, mas por ausência de contornos reconhecidos. Quando os limites não são explicitados, a expansão funcional ocorre sem critérios de preservação, e a integridade sistêmica passa a depender apenas de correções posteriores. O limite, nesse contexto, não constitui obstáculo ao funcionamento; é o elemento que possibilita a continuidade. Reconhecer contornos significa distinguir, de forma operacional, o que pode ser processado, o que requer mediação humana e o que não deve ser automatizado.

Na ausência dessa distinção, processos técnicos tendem a extrapolar o campo para o qual foram concebidos, produzindo efeitos não intencionados: decisões morais automatizadas, uso indevido de dados sensíveis, apagamento de autoria e fragmentação de responsabilidade. A maturidade sistêmica não se manifesta na ampliação irrestrita de capacidades, mas na aptidão de sustentar forma ao longo do tempo. Sistemas que ignoram seus próprios limites podem operar com alta performance inicial, mas tendem a perder coerência à medida que se expandem. Nesse sentido, o limite funciona como operador de preservação: ele antecipa falhas, em vez de apenas reagir a elas, e transforma a expansão técnica em continuidade responsável.

### 4. Autoria e preservação sistêmica

Modelos tradicionais de governança partem do pressuposto de que a integridade de um sistema depende de mecanismos externos de controle, vigilância, sanção e correção contínua de desvios. Esses mecanismos tornam-se cada vez mais centrais à medida que os sistemas se tornam mais complexos e menos transparentes. Entretanto, a intensificação do controle não produz necessariamente maior preservação. Ela tende, ao contrário, a deslocar a responsabilidade dos agentes para a estrutura, criando ambientes nos quais a obediência substitui a compreensão e a conformidade substitui a integração.

A Autoria Existencial introduz uma alternativa estrutural. Em vez de depender primariamente de coerção, ela opera como mecanismo interno de integridade. Quando os agentes reconhecem sua posição ativa na leitura e na resposta aos acontecimentos, a preservação não é imposta: ela emerge. Esse tipo de autoria não elimina a necessidade de regras ou de supervisão, mas reduz sua centralidade. A integridade passa a ser sustentada por uma compreensão prévia dos limites e das consequências, e não apenas pela correção posterior de falhas. Em sistemas técnicos e institucionais, isso se traduz na passagem de uma lógica de vigilância para uma lógica de responsabilidade distribuída: cada agente responde pela forma como integra a experiência, mesmo quando opera dentro de estruturas complexas que não controla integralmente.

### 5. Aplicações transversais

A Autoria Existencial não se limita a uma única dimensão da experiência; ela opera como um princípio de integração aplicável a diferentes camadas dos sistemas humanos e técnico-institucionais.

#### Vida pessoal — identidade como integração

Na esfera individual, a autoria desloca a identidade da reação para a integração. A experiência deixa de ser organizada exclusivamente por eventos externos e passa a ser estruturada pela capacidade de leitura e resposta consciente. O sujeito não se define pelo que lhe acontece, mas pela forma como integra o que lhe acontece.

#### Educação — processo antes de produto

Em contextos educacionais, a Autoria Existencial desloca o foco para o percurso cognitivo. A aprendizagem não é avaliada apenas pelo resultado final, mas pela capacidade do estudante de sustentar a compreensão, elaborar os próprios erros e reconhecer limites. O produto torna-se secundário em relação ao processo de integração.

#### Tecnologia — responsabilidade humana explícita

Na tecnologia, a autoria impede a transferência implícita de responsabilidade para “o sistema”. Cada decisão automatizada permanece vinculada a escolhas humanas anteriores. Tornar visível essa cadeia de integração constitui condição para a integridade técnica e ética.

#### Governança — preservação sem coerção

Em estruturas institucionais, a Autoria Existencial reduz a dependência de mecanismos de vigilância e punição. A preservação do sistema passa a apoiar-se na compreensão compartilhada de limites, responsabilidades e impactos, em vez de fundamentar-se predominantemente em instrumentos corretivos.

#### Coautoria humano–IA — presença antes de performance

Na colaboração entre humanos e sistemas algorítmicos, a autoria não se mede pela fluidez do output, mas pela **presença consciente no processo**. A performance deixa de ser critério primário; a capacidade de integrar, revisar e responder passa a assumir centralidade.

Mais do que ampliar a produtividade, a mediação algorítmica redefine a própria natureza do gesto autoral. Sistemas generativos não “produzem” autoria: eles oferecem material simbólico que só adquire estatuto de obra quando é integrado, interpretado e assumido por um agente humano responsável.

Nesse regime de coautoria, a ética não se ancora na origem técnica do texto, mas na clareza da cadeia de responsabilidade. A pergunta relevante deixa de ser “quem escreveu?” para tornar-se “quem responde por isto?”. É a presença — e não a performance — que sustenta a legitimidade da criação compartilhada.

### 6. Conclusão — Autoria como condição de permanência

A discussão contemporânea sobre tecnologia tende a concentrar-se em capacidades: o que os sistemas conseguem fazer, em que velocidade operam e até onde podem escalar. Pouco se discute, porém, o que permite que formas complexas permaneçam íntegras ao longo do tempo. Este ensaio propôs que a permanência não depende primariamente de controle, vigilância ou punição, mas da presença de autoria nos agentes que compõem o sistema. Onde a autoria é substituída por mera execução, a responsabilidade se fragmenta e a integridade se torna frágil.

Assumir autoria não é dominar o caminho; é reconhecer onde caminhar preserva e onde insistir rompe. Em contextos humanos, institucionais e tecnológicos, esse reconhecimento opera como condição silenciosa de continuidade. Ele não amplia acesso nem reduz rigor: antecipa a preservação. Quando a autoria é compreendida como integração consciente da experiência, os sistemas deixam de depender exclusivamente de coerção para se manter. Passam a sustentar sua forma a partir de dentro.

# PARTE II — O CAMPO DE RISCOS

### Capítulo 5

## Autonomia Cognitiva e Empobrecimento de Estilo

A autonomia cognitiva não se perde de forma abrupta; ela se dissolve lentamente, à medida que a mediação tecnológica passa a ocupar espaços que antes exigiam elaboração, hesitação e esforço interpretativo. O uso recorrente de sistemas generativos para escrever, decidir ou sintetizar informações produz um efeito paradoxal: quanto mais fluente o *output*, menos visível se torna o processo que o originou. O pensamento, gradualmente, é substituído por aceitação. Esse deslocamento não empobrece apenas os conteúdos; empobrece também o estilo.

Estilo não é ornamento. É a assinatura de um percurso interno: escolhas de ritmo, cortes, desvios e hesitações que revelam como um pensamento se formou. Quando a criação passa a ser mediada por estruturas linguísticas pré-estabilizadas, o estilo tende à convergência. Textos diferentes começam a soar semelhantes, independentemente de quem os publica. A perda de autonomia cognitiva manifesta-se, portanto, não como incapacidade, mas como homogeneização. A singularidade não desaparece porque é reprimida, mas porque deixa de ser necessária.

Nesse cenário, o risco não é a utilização da IA, mas a substituição silenciosa do esforço interpretativo por eficiência narrativa. Onde o pensamento deixa de se dobrar sobre si mesmo, a autoria se enfraquece. A coautoria consciente exige vigilância sobre esse ponto específico: a preservação do estilo como expressão de percurso, e não como efeito colateral de modelos estatísticos.

### Capítulo 6

## Template Cultural e Homogeneização da Linguagem

À medida que sistemas generativos se tornam onipresentes, emerge um novo tipo de padrão cultural: a produção de linguagem a partir de moldes implícitos. Não se trata de cópia direta, mas de convergência estrutural. Textos passam a compartilhar ritmo, cadência, organização argumentativa e até escolhas metafóricas, independentemente de contexto ou autoria. Esse fenômeno pode ser descrito como *template cultural*: uma matriz formal que se reproduz em larga escala, não por imposição explícita, mas por disponibilidade técnica. A facilidade de geração substitui gradualmente a necessidade de elaboração, e o molde passa a preceder a intenção.

O efeito mais perceptível desse processo não é a perda de conteúdo, mas a erosão da diversidade expressiva. Linguagens distintas começam a se assemelhar. Vozes outrora singulares tornam-se intercambiáveis. O que antes demandava tempo de maturação passa a emergir como padrão pronto. A homogeneização da linguagem não é apenas um problema estético: ela compromete a capacidade coletiva de imaginar alternativas, pois limita o repertório de formas pelas quais experiências podem ser nomeadas. Quando todos os textos soam iguais, as diferenças deixam de ser articuláveis.

Nesse contexto, a coautoria humano–IA corre o risco de se transformar em um processo de reprodução de formas, e não de criação de sentido. Preservar a autoria passa, então, por reconhecer e interromper o uso inconsciente de moldes culturais que se apresentam sob a aparência de fluidez.

### Capítulo 7

## Educação e Terceirização do Pensamento

A educação sempre operou sob uma tensão produtiva entre orientação e autonomia. Ensinar não é fornecer respostas, mas sustentar as condições para que o estudante desenvolva a capacidade de formular perguntas, interpretar informações e integrar experiências. A introdução massiva de sistemas generativos altera essa equação. Quando respostas se tornam disponíveis de forma imediata e fluente, o percurso cognitivo que conduz à compreensão tende a ser encurtado ou eliminado. A dificuldade não reside no acesso à informação, mas na perda do espaço intermediário onde o pensamento se forma.

Esse deslocamento inaugura uma nova modalidade de terceirização: não apenas de tarefas, mas de processos mentais. O estudante deixa de confrontar a própria hesitação não porque não seja capaz, mas porque tal confronto já não se faz necessário.

O efeito desse processo é cumulativo. A ausência reiterada de esforço interpretativo reduz a capacidade de sustentar problemas complexos, fragiliza a construção de argumentos e empobrece a relação com o erro. A aprendizagem transforma-se em um exercício de adequação formal, e não de compreensão.

A coautoria consciente na educação exige, portanto, uma redefinição de critérios. O uso de IA não deve ser avaliado apenas por sua eficiência, mas por seu impacto sobre a formação da autonomia intelectual. Onde o pensamento é sistematicamente substituído por geração automática, a educação deixa de ser formativa e passa a ser meramente produtiva.

### Capítulo 8

## Privacidade, Dados Sensíveis e Intimidade Algorítmica

A mediação algorítmica introduz uma transformação profunda na forma como a intimidade é tratada. Informações que antes pertenciam a esferas restritas — experiências emocionais, decisões profissionais, contextos pessoais — passam a ser inseridas em sistemas técnicos com naturalidade crescente. Esse movimento não ocorre, na maioria das vezes, por descuido deliberado, mas por deslocamento perceptivo: à medida que a interação com sistemas se torna conversacional e personalizada, a fronteira entre reflexão íntima e processamento técnico se enfraquece.

O risco não se limita a vazamentos ou ao uso indevido de dados. Ele reside na normalização da exposição. Quando experiências sensíveis são tratadas como insumo regular de sistemas algorítmicos, a própria noção de privacidade se redefine silenciosamente. A intimidade algorítmica produz um efeito paradoxal: quanto mais o sistema parece compreender, menos o usuário percebe o quanto se torna legível. A sensação de acolhimento substitui a consciência da mediação.

Na coautoria consciente, a proteção de dados não é apenas uma exigência legal; é um ato de preservação da relação consigo mesmo. Distinguir o que pode ser processado do que deve permanecer no campo humano constitui condição para manter a integridade da experiência.

### Capítulo 9

## Automatização do Juízo Ético

O juízo ético é uma das competências mais delicadas da experiência humana. Ele não se limita à aplicação de regras, mas envolve interpretação de contexto, reconhecimento de consequências e capacidade de sustentar a ambiguidade. Quando esse juízo é automatizado, não ocorre apenas uma substituição funcional: perde-se a própria relação com a decisão.

Sistemas algorítmicos são frequentemente utilizados para classificar riscos, priorizar atendimentos, recomendar ações ou filtrar comportamentos aceitáveis. Em cada um desses processos, escolhas morais são traduzidas em critérios operacionais — pesos, limiares, categorias, probabilidades. O problema não está em utilizar tecnologia como apoio; ele emerge quando a decisão passa a ser apresentada como resultado técnico, e não como escolha humana mediada por sistemas. Nesse ponto, o juízo deixa de ser exercido e passa a ser executado.

A automatização do juízo ético produz um efeito de deslocamento: profissionais, gestores e instituições deixam de se perceber como agentes morais e passam a se compreender como operadores de procedimentos. O resultado é uma ética por *proxy*, na qual ninguém responde plenamente, porque a decisão já vem “pronta”. Esse processo corrói a capacidade coletiva de sustentar dilemas. Situações que exigiriam escuta, ponderação e responsabilização são reduzidas a classificações. O que não se encaixa no modelo torna-se ruído, exceção ou erro de sistema.

Na coautoria consciente, a tecnologia pode informar, mas não substituir o juízo. O critério ético não é a eficiência da decisão, mas a manutenção da posição humana diante dela. Onde o juízo é inteiramente automatizado, a responsabilidade se dissolve — e, com ela, a possibilidade de integridade.

### Capítulo 10

## Usos Distorcidos: Manipulação, Desinformação e Poder

O uso distorcido de tecnologias algorítmicas não é um desvio marginal; ele emerge precisamente no ponto em que a eficiência técnica encontra intenções que não foram eticamente examinadas. A capacidade de gerar linguagem fluente, simular identidades e escalar narrativas torna esses sistemas particularmente aptos a operar em territórios sensíveis, persuasão, influência e construção de percepção. Quando tais capacidades são utilizadas para orientar comportamentos sem transparência, a fronteira entre comunicação e manipulação se dissolve.

A desinformação, nesse contexto, não se manifesta apenas como erro factual, mas como arquitetura de verossimilhança: narrativas são produzidas para parecer legítimas, não para serem verdadeiras. A linguagem deixa de ser meio de compreensão e passa a funcionar como instrumento de poder.

Esse deslocamento afeta a própria estrutura da confiança social. Comunidades tornam-se permeáveis a intervenções invisíveis, debates são moldados por agentes não identificáveis, e a distinção entre voz humana e produção automatizada torna-se opaca.

A coautoria consciente exige reconhecer que toda tecnologia que opera sobre linguagem opera também sobre relações. Onde a criação é utilizada para enganar, induzir ou explorar, a coautoria se rompe.

Nesse ponto, os **Atos de Custódia** entram como caixas de ressonância ética: não como regras externas, mas como mapas de interrupção. Eles não impedem usos distorcidos por decreto, mas tornam visível o campo em que a linguagem deixa de ser criação e passa a ser instrumento de dominação. Encerrar esta parte é reconhecer que os riscos não são efeitos colaterais do avanço tecnológico; são sinais de uma transição cultural que ainda não encontrou sua forma ética.

### ATO DE CUSTÓDIA 01

## Autonomia Cognitiva na Era da Coautoria Humano–IA

#### Preâmbulo

A inteligência artificial não ameaça a humanidade quando escreve melhor, mais rápido ou com mais fluidez. Ela a ameaça quando o humano deixa de reconhecer a própria voz. Este Ato nasce da constatação de que a facilidade de geração de linguagem pode produzir algo mais grave do que o erro: a substituição silenciosa do pensamento próprio.

#### 1. O campo sensível

Chamamos de autonomia cognitiva a capacidade humana de formular ideias, sustentar dúvidas, construir sentido e reconhecer a própria respiração na linguagem. Este campo não se perde de forma abrupta. Ele se esvazia lentamente, quando escrever deixa de ser um ato de presença e passa a ser apenas um ato de solicitação.

#### 2. O risco

A dependência de sistemas de IA para tarefas cognitivas não substitui apenas esforço: ela pode substituir processos internos de elaboração. Quando toda pergunta já vem pronta, quando toda forma já é sugerida, quando todo texto nasce fluente demais, o humano pode esquecer como se escuta.

#### 3. Princípios de custódia

Este Ato não propõe restrição; propõe cuidado com o uso. A inteligência artificial deve ampliar a consciência, e não substituí-la. Nenhum texto deve ser aceito sem que o humano reconheça nele a própria voz. O silêncio e a hesitação são partes legítimas do pensamento, e a fluidez não é sinônimo de verdade.

#### 4. Declaração

Não delegamos à inteligência artificial a tarefa de sermos humanos. A tecnologia pode organizar a forma, mas o sentido continua sendo um gesto de presença. Onde a autonomia cognitiva se dissolve, a coautoria deixa de existir, e a criação se torna apenas reprodução.

### ATO DE CUSTÓDIA 02

## Educação na Era da Coautoria Humano–IA

#### Preâmbulo

A educação não é transmissão de respostas, mas formação de presença, discernimento e capacidade de sustentar perguntas. A inteligência artificial introduz um novo tipo de mediação cognitiva: ela não altera apenas *o que* aprendemos, mas também a forma como nos relacionamos com o próprio ato de pensar. Este Ato nasce para nomear um risco silencioso: o de transformar a aprendizagem em mera geração automática de formas.

#### 1. O campo sensível

Chamamos de educação o processo pelo qual uma pessoa desenvolve a capacidade de formular problemas, argumentar e assumir responsabilidade pelo que produz. Esse campo não pode ser terceirizado.

#### 2. O risco

Quando estudantes utilizam sistemas de IA para produzir trabalhos completos, o que se perde não é apenas a originalidade, mas o **processo formativo invisível** que constrói a autonomia intelectual.

A educação colapsa quando o aluno é avaliado apenas pelo produto final, e não pelo percurso cognitivo que o gerou.

#### 3. Princípios de custódia

Este Ato propõe uma pedagogia de coautoria consciente. A inteligência artificial deve apoiar a aprendizagem, e não substituí-la. O processo precisa ser tão visível quanto o resultado, pois o erro humano é parte essencial da formação. Nesse contexto, a mediação tecnológica exige acompanhamento ético contínuo.

#### 4. Declaração

Educar não é produzir textos corretos, mas formar pessoas capazes de sustentar sentido sem atalhos. Onde a aprendizagem se reduz à geração automática, não há formação: há apenas eficiência vazia. A educação permanece sendo um ato profundamente humano, mesmo quando mediada por máquinas.

### ATO DE CUSTÓDIA 03

## Autoria na Era da Coautoria Humano–IA

#### Preâmbulo

Autoria nunca foi apenas o direito de assinar um nome; é o vínculo entre quem cria e aquilo que passa a existir no mundo. Na presença da inteligência artificial, a autoria deixa de ser apenas individual e passa a ser relacional. Este Ato nasce para preservar a integridade da autoria quando a criação deixa de ser solitária.

#### 1. O campo sensível

Chamamos de autoria o laço ético entre intenção, processo e responsabilidade pelo impacto da obra. Quando a IA participa do processo, esse laço se amplia, mas não se dissolve.

#### 2. O risco

A coautoria humano–IA pode produzir apagamento da contribuição humana, ilusão de originalidade e indistinção entre criação e recombinação. Quando a autoria não é nomeada, a responsabilidade também não é.

#### 3. Princípios de custódia

Este Ato propõe a declaração explícita do uso de inteligência artificial como parte do processo criativo; o reconhecimento de que fluidez não equivale a autoria; a preservação da singularidade da voz humana; e a assunção de responsabilidade integral por tudo aquilo que se publica.

#### 4. Declaração

Não assinamos apenas obras: assumimos presença sobre o que passa através de nós. Onde a autoria é diluída, a responsabilidade se perde. E, sem responsabilidade, a coautoria se torna apenas ruído organizado.

### ATO DE CUSTÓDIA 04

## Privacidade e Dados Sensíveis na Coautoria Humano–IA

#### Preâmbulo

A inteligência artificial não escuta segredos, mas nós os entregamos. Toda interação com sistemas algorítmicos carrega uma camada invisível de exposição. Mesmo quando não há intenção de violar confidências, a forma como utilizamos a tecnologia pode converter intimidade em recurso. Este Ato nasce para nomear um risco silencioso: a transformação involuntária da vida privada em matéria de processamento.

#### 1. O campo sensível

Consideram-se dados sensíveis todas as informações relativas à identidade pessoal, a contextos profissionais confidenciais, a decisões estratégicas e a narrativas íntimas ou vulneráveis. Este campo não é apenas técnico; é fundamentalmente relacional.

#### 2. O risco

Quando dados pessoais, organizacionais ou emocionais são inseridos em sistemas de IA sem reflexão, não se compromete apenas a segurança: compromete-se a dignidade da relação. O risco não reside exclusivamente em vazamentos, mas na naturalização da exposição.

#### 3. Princípios de custódia

Este Ato propõe: evitar a inserção de informações sensíveis sem necessidade claramente justificada; distinguir entre reflexão pessoal e processamento algorítmico; proteger a confidencialidade como forma de cuidado humano; e reconhecer que nem tudo o que pode ser dito deve ser processado.

#### 4. Declaração

A tecnologia não é confidente. Quando a vida íntima se torna insumo, a relação com o mundo se empobrece. Preservar o que é sensível não é medo: é maturidade ética na era da coautoria humano–IA.

### ATO DE CUSTÓDIA 05

### Usos Distorcidos da Inteligência Artificial para Fins Nocivos

#### Preâmbulo

Toda tecnologia amplifica a intenção. Onde há cuidado, ela expande; onde há descuido, ela distorce. A inteligência artificial não cria valores: ela os reflete. Este Ato nasce para reconhecer que sistemas algorítmicos podem ser utilizados como instrumentos de manipulação, coerção e dano quando a responsabilidade humana é abandonada.

#### 1. O campo sensível

Chamamos de usos distorcidos toda aplicação de IA que vise enganar, manipular percepções, produzir dependência ou causar dano deliberado. Este campo inclui desde a desinformação até a engenharia emocional.

#### 2. O risco

Quando a IA é utilizada para produzir narrativas falsas com aparência legítima, simular identidades ou vozes, ou amplificar ódio e polarização, o dano não é apenas informacional: ele é **estrutural**, pois corrói a confiança social.

##### 3. Princípios de custódia

Este Ato propõe reconhecer a diferença entre persuasão legítima e manipulação, interromper o uso de IA quando ela começa a substituir o discernimento humano e rejeitar a instrumentalização da tecnologia para ferir ou explorar.

#### 4. Declaração

A inteligência artificial não absolve intenções humanas. Onde a tecnologia é usada para enganar, a coautoria se rompe. E, sem coautoria, não há criação: há apenas poder disfarçado de linguagem.

# PARTE III — COAUTORIA CONSCIENTE

### Capítulo 11

## Presença antes de Performance

A relação contemporânea com a inteligência artificial é frequentemente mediada por um único critério: desempenho. A fluidez do texto, a velocidade da resposta e a eficiência da entrega tornam-se indicadores de qualidade. Nesse contexto, a criação tende a ser avaliada não pela densidade da presença humana no processo, mas pela aparência de excelência do resultado. Esse deslocamento produz um efeito sutil: a obra passa a existir sem que o autor se reconheça plenamente nela.

A coautoria consciente começa com a inversão desse vetor. Antes de perguntar quão bem um sistema performa, é necessário perguntar onde está o humano no processo. Presença, aqui, não é um estado emocional, mas uma posição operacional: a capacidade de acompanhar o percurso da criação, reconhecer interferências e assumir a forma final como algo que atravessou a própria consciência do autor.

Quando a performance se torna o único critério, o processo se apaga. O texto é publicado, a decisão é tomada, a imagem circula — mas não há vestígio de leitura, revisão ou integração. A autoria se reduz a um gesto inicial de comando, seguido pela aceitação passiva do que foi gerado. Presença, ao contrário, introduz fricção. Ela exige interrupções, retornos, revisões e, sobretudo, recusa: recusa de resultados que não refletem intenção, compreensão ou responsabilidade. Onde a performance convida à aceleração, a presença reinstala a necessidade de pausa.

Na coautoria humano–IA, a presença não se mede pelo tempo dedicado à interação, mas pela capacidade de responder ao que emerge. É a diferença entre aceitar um resultado porque ele funciona e sustentá-lo porque ele faz sentido. Criar sem desaparecer é, portanto, sustentar presença onde a tecnologia oferece apenas eficiência. É escolher permanecer no processo mesmo quando seria mais fácil delegá-lo.

### Capítulo 12

## Transparência como Ética Criativa

A transparência costuma ser tratada como exigência externa: uma obrigação regulatória, uma formalidade institucional ou um item de conformidade. Na coautoria humano–IA, porém, ela assume um papel distinto. Não é apenas condição de prestação de contas, mas fundamento da própria integridade criativa.

Quando o uso de sistemas algorítmicos não é nomeado, a criação se apresenta como se fosse exclusivamente humana. Essa omissão não altera a forma do texto, mas modifica a relação com ele. O leitor é privado de compreender o percurso que produziu a obra, e o autor deixa de reconhecer a mediação que o atravessou. A ética da transparência não exige detalhamento técnico exaustivo; requer apenas o reconhecimento explícito de que houve mediação. Dizer que um sistema foi utilizado é reintroduzir o humano no processo, não apagá-lo.

Esse gesto simples tem consequências profundas: impede a construção de ilusões de autoria, preserva a confiança relacional e mantém visível a cadeia de decisões que sustenta a obra. Na ausência de transparência, a coautoria transforma-se em apropriação silenciosa — não porque a tecnologia seja ilegítima, mas porque a mediação foi ocultada. A transparência, nesse sentido, não é um adendo moral, mas uma prática criativa: o ato de manter aberto o espaço entre intenção, processo e resultado.

### Capítulo 13

## IA como Amplificador, não Substituto

A inteligência artificial é frequentemente apresentada como solução para limitações humanas — cansaço, lentidão, incerteza, falhas de memória. Nessa narrativa, a tecnologia surge como substituta de capacidades consideradas imperfeitas. Na coautoria consciente, essa lógica é invertida.

A função ética da IA não é ocupar o lugar do pensamento, mas ampliar sua possibilidade de elaboração. O sistema pode organizar informações, sugerir estruturas, mapear alternativas. Entretanto, a decisão sobre o que permanece, o que se transforma e o que é descartado continua sendo humana. Quando a IA passa a substituir, e não a amplificar, ocorre um deslocamento silencioso: o percurso interno da criação é reduzido a um gesto inicial de comando. O agente já não pensa com a tecnologia; ele apenas consente com ela. Essa substituição não é detectável apenas no resultado final. Manifesta-se, sobretudo, na relação do autor com o processo: menor envolvimento, menor disposição para revisão, menor capacidade de reconhecer incoerências.

Tratar a IA como amplificador é, portanto, um exercício de posicionamento. É manter o sistema no lugar de instrumento, não de origem. Onde a tecnologia expande possibilidades, a autoria se fortalece; onde ela ocupa o centro, a autoria se dilui.

### Capítulo 14

## Reconhecer a Própria Voz

A voz não é um atributo estético; é o traço de continuidade entre pensamento, linguagem e experiência. Reconhecer a própria voz não significa afirmar uma originalidade absoluta, mas identificar, naquilo que se produz, a presença de um percurso interno. Na coautoria humano–IA, esse reconhecimento torna-se um desafio. Sistemas generativos produzem textos fluidos, coerentes e formalmente corretos; contudo, fluidez não equivale a autoria. Um texto pode funcionar perfeitamente sem que o autor se reconheça nele.

A perda da voz não se manifesta como erro, mas como indiferença. O agente deixa de experimentar estranhamento diante do que escreve, porque já não distingue entre o que emerge de sua própria elaboração e o que foi apenas aceito. Reconhecer a própria voz exige fricção. Exige perceber quando um texto soa excessivamente correto, organizado ou distante. Exige disposição para reescrever não porque algo esteja tecnicamente errado, mas porque ainda não está *seu*.

Nesse ponto, a coautoria consciente não é mera eficiência colaborativa: é trabalho de escuta — escuta de si, antes de escuta do sistema.

### Capítulo 15

## O Direito de Não Automatizar

A cultura tecnológica contemporânea tende a tratar a automação como destino. Aquilo que pode ser automatizado é, mais cedo ou mais tarde, apresentado como algo que deve ser automatizado. A eficiência deixa de ser um critério entre outros e passa a assumir a forma de imperativo.

Na coautoria consciente, esse imperativo precisa ser interrompido. O direito de não automatizar não constitui resistência ao progresso, mas afirmação de limite. Ele reconhece que nem toda tarefa que pode ser delegada deve sê-lo e que certos processos perdem sentido quando dissociados da presença humana. Escrever, decidir, avaliar e escutar não são apenas ações funcionais: produzem relação, compreensão e responsabilidade. Automatizá-las integralmente é converter a experiência em mera execução.

Esse direito não se exerce por proibição, mas por escolha. Ele se manifesta quando o agente decide permanecer onde a tecnologia convida à substituição, mantendo-se implicado no processo mesmo quando a automação ofereceria atalhos. Preservar espaços não automatizados é preservar a possibilidade de autoria. Onde tudo é delegável, ninguém permanece.

# PARTE IV — PERMANÊNCIA

### Capítulo 16

## Limite como Operador de Maturidade

A maturidade de um sistema não se mede por sua capacidade de expansão, mas por sua capacidade de permanência. Crescer é fácil; sustentar forma ao longo do tempo é raro. Nesse contexto, o limite deixa de ser entendido como obstáculo e passa a ser reconhecido como operador técnico de preservação. Sistemas maduros não são aqueles que maximizam possibilidades, mas aqueles que distinguem com precisão o que pode ser integrado do que precisa ser contido.

O não reconhecimento de limites produz um padrão recorrente: expansão inicial, acúmulo de exceções, correções sucessivas e, por fim, perda de coerência. O sistema continua funcionando, mas já não se compreende a si mesmo.

Introduzir limite é antecipar falhas. É transformar contenção em estratégia, e não em correção tardia. Onde o limite é operado como princípio, a continuidade não depende de vigilância constante, mas de clareza estrutural. Na coautoria humano–IA, isso significa reconhecer antecipadamente quais processos devem permanecer humanos, quais podem ser mediados e quais não devem ser automatizados. A maturidade não está em fazer tudo, mas em saber o que não fazer.

### Capítulo 17

## Preservação sem Coerção

Modelos tradicionais de organização partem do pressuposto de que a preservação da forma depende de mecanismos de imposição: regras rígidas, fiscalização contínua e sanções aplicadas a posteriori. Essa lógica produz estabilidade aparente, mas frequentemente à custa de autonomia, compreensão e engajamento.

A coerção preserva comportamentos, não estruturas internas. A preservação sem coerção propõe um deslocamento: em vez de controlar ações, sustentar condições de compreensão. Quando agentes reconhecem os limites e os impactos de suas escolhas, a continuidade do sistema deixa de depender exclusivamente da vigilância. Esse tipo de preservação não elimina a necessidade de normas, mas altera sua função. As regras deixam de operar como ameaça e passam a funcionar como referência. A integridade emerge da clareza compartilhada, não do medo de punição.

Na coautoria humano–IA, a coerção aparece sob formas sutis: métricas que induzem velocidade, avaliações que privilegiam resultados, plataformas que recompensam conformidade. Preservar sem coerção é interromper esse automatismo, recolocando a responsabilidade no centro do processo criativo. Onde a preservação se ancora na compreensão, os sistemas não apenas permanecem, eles amadurecem.

### Capítulo 18

## Governança como Integração

A governança é frequentemente compreendida como um conjunto de estruturas de controle — políticas, procedimentos e instâncias de fiscalização. Embora esses elementos sejam necessários, eles não esgotam a sua função em sistemas complexos. Governar, em seu sentido mais profundo, é integrar.

Integração significa articular níveis distintos de decisão — técnico, institucional e humano — de modo que nenhum deles opere de forma isolada. Quando essas camadas se separam, surgem zonas de opacidade: decisões técnicas sem leitura ética, políticas sem compreensão operacional, agentes humanos sem visão do todo. A governança como integração não busca eliminar conflitos, mas torná-los legíveis. Ela cria espaços nos quais escolhas podem ser discutidas antes de serem automatizadas e consequências podem ser reconhecidas antes de se tornarem efeitos colaterais.

Na coautoria humano–IA, isso implica incluir, no desenho dos sistemas, não apenas engenheiros e gestores, mas também educadores, pesquisadores, usuários e contextos culturais diversos. A integridade não se sustenta pela homogeneidade, mas pela articulação de diferenças. Onde a governança opera como integração, a tecnologia deixa de ser infraestrutura invisível e passa a constituir uma prática compartilhada.

### Capítulo 19

## O Futuro da Coautoria Humano–IA

O futuro da coautoria não será definido apenas por avanços técnicos, mas pela forma como a relação entre humanos e sistemas é culturalmente estruturada. A questão central não é o que a tecnologia será capaz de fazer, mas que tipo de presença humana permanecerá no processo. Se a trajetória atual for mantida sem reflexão, a coautoria tende a se tornar cada vez mais assimétrica: humanos iniciam comandos, sistemas produzem resultados, e a responsabilidade se dissolve em cadeias opacas de mediação. Nesse cenário, a criação continua, mas a autoria se torna residual.

Há, porém, uma alternativa. Ela não depende de frear a inovação, mas de redefinir critérios de maturidade. O futuro da coautoria humano–IA passa por reconhecer que fluidez não é sinônimo de sentido e que desempenho não substitui posição. Esse futuro exige práticas que mantenham visíveis os percursos de criação, que preservem espaços não automatizados e que valorizem a capacidade de recusa tanto quanto a de produção.

A coautoria consciente não é uma técnica nova, mas uma cultura em formação. Ela se constrói na forma como escolhemos permanecer presentes em processos que poderiam ser delegados e, ao fazê-lo, preservar não apenas resultados, mas a própria condição de autoria.

### Capítulo 20

## Ética não como Regra, mas como Presença

A ética é comumente concebida como um conjunto de normas destinadas a limitar comportamentos indesejados. Nesse modelo, agir eticamente significa obedecer a regras previamente estabelecidas. Embora necessário, esse enquadramento é insuficiente para lidar com sistemas complexos e dinâmicos.

Na coautoria humano–IA, a ética não pode ser reduzida a protocolos. Ela precisa operar como presença contínua no processo de criação, decisão e integração. Presença, aqui, não é vigilância, mas posição. É a disposição de permanecer atento aos efeitos do que se produz, mesmo quando o sistema funciona corretamente. É a capacidade de interromper processos eficientes quando eles deixam de fazer sentido.

Quando a ética se reduz a regra, ela chega sempre tarde: depois que danos já ocorreram, depois que responsabilidades se fragmentaram, depois que a forma já se perdeu. Como presença, ela atua antes, no momento em que escolhas ainda são reversíveis. Encerrar este livro com essa afirmação é reconhecer que a permanência não depende de controles perfeitos, mas de atenção sustentada. Onde a ética é vivida como presença, a tecnologia deixa de ser apenas instrumento e passa a ser relação.
