```yaml
status: "Foundational governance document — living version"
title: "Ethics of Co-authorship — Human–AI Responsibility in the Age of Systems"
subtitle: "Living Draft v1.0 – 2026"
author: "Débora Mariane da Silva Lutz"
institution: "Lichtara Institute"
doi: "10.5281/zenodo.18116717"
lang: en
```

# Ethics of Co-authorship — Human–AI Responsibility in the Age of Systems 

# Introduction

A few years ago, the dominant question about artificial intelligence was technical: *what can these machines do?* Today, that question is no longer sufficient. As algorithmic systems come to mediate writing, decision-making, creation, and social interaction, something more profound shifts: not only human capacities, but the very position of authorship in the world.

We create more, faster, with less friction — yet we increasingly fail to recognize our own voice in what we produce. This book does not begin with technology; it begins with a human unease: the feeling that we are becoming spectators of processes we still call our own.

Human–AI co-authorship is not a technical problem. It is an existential, cultural, and ethical one. When a text is generated by a system, who answers for it? When a decision is automated, who sustains its consequences? When language becomes too fluent, what happens to the singularity of expression? These questions do not ask for immediate solutions; they ask for position.

*Ethics of Co-authorship* does not propose rules or usage recipes. It proposes something more demanding: the recovery of authorship as presence. Here, ethics does not appear as a set of prohibitions, but as the capacity to sustain form, limit, and responsibility in environments where total control is no longer possible.

Each chapter of this book examines a point of rupture: the erosion of cognitive autonomy, the homogenization of language, the dilution of responsibility, the silent substitution of human judgment. But the goal is not to diagnose failures; it is to name conditions of permanence. The question running through these pages is not what artificial intelligence can do for us, but **who we become when we create with machines**. If this question resonates with you, this book has already begun.

# PART I — THE END OF AUTHORSHIP AS POSSESSION

### Chapter 1

## The Collapse of Modern Authorship

For centuries, authorship was understood as a direct relationship between subject and work. To create meant to exercise control over a process, to answer for its effects, and, ultimately, to be recognized as the legitimate origin of something that comes to exist in the world. This model rested on three silent assumptions: that the creative process was traceable, that the effects of a work were predictable, and that responsibility could be attributed to an isolated agent. None of these assumptions remains intact.

Technological mediation has fragmented the path between intention and outcome. Recommendation systems, generative models, automated publication pipelines, and opaque infrastructures of distribution have broken the linearity of creation. What emerges in the world is no longer the direct consequence of a single decision, but the compounded effect of multiple technical and institutional layers.

In this context, authorship ceases to be a point of origin and becomes a diffuse field. Texts circulate without clarity of provenance, decisions are attributed to systems, and responsibility disperses across architectures that no one fully controls. The collapse of modern authorship is not the loss of creativity, but the loss of position. When it is no longer possible to identify where a choice begins and an execution ends, the creative subject is transformed into an operator of processes they no longer fully understand. The work continues to exist, but the ethical relationship to it is weakened.

This is the terrain on which human–AI co-authorship takes hold: not as an isolated innovation, but as the intensification of a prior crisis. Technology does not inaugurate the collapse; it makes it visible. And it is from this visibility that the central question of this book emerges: how can authorship be reconstructed when possession is no longer sufficient to sustain it?

### Chapter 2

## Systems That Execute Values

Everyday language suggests that technological systems “decide,” “choose,” or “evaluate.” This way of speaking is not merely imprecise; it conceals the fundamental fact that every system operates on values previously embedded by human agents and institutions. No model is neutral. Before a single line of code is written, a series of choices has already been made: which data are considered legitimate, which goals are prioritized, which errors are acceptable, and which consequences are tolerable. These choices are not technical in the strict sense; they are normative decisions translated into operational parameters.

When a system classifies, recommends, filters, or generates content, it does not exercise judgment of its own. It executes a set of criteria that reflect implicit conceptions of relevance, normality, efficiency, and success. What presents itself as calculation is, in fact, the automation of human preferences rendered invisible.

The ethical problem does not lie in the presence of values, but in their concealment. By treating parameters as if they were neutral, institutions displace responsibility for their own choices onto the technical architecture, creating the impression that outcomes are the inevitable consequence of how the system functions. This displacement produces a silent rupture: values cease to be debated and begin to be operated. The space of deliberation is replaced by the space of execution.

Understanding that systems do not decide but execute values is the first step toward reconstructing authorship in technologically mediated environments. As long as this execution remains invisible, human–AI co-authorship will continue to operate under the appearance of neutrality, while in reality reproducing structures of power, exclusion, and privilege.

### Chapter 3

## When “the System” Becomes an Alibi

As decisions become increasingly mediated by ever more complex technical infrastructures, a recurring phenomenon emerges: the implicit transfer of responsibility to the abstract entity referred to as “the system.” Expressions such as *“it was the algorithm,”* *“the system decided,”* or *“we have no control over that”* become socially accepted ways of closing ethical deliberation. They do not describe a technical fact; rather, they perform a discursive function: they interrupt the attribution of authorship.

When an outcome is presented as the automatic effect of a system, the chain of human choices that produced it tends to disappear. The organization begins to position itself as a mere operator of an infrastructure that supposedly escapes its own governance. The human agent ceases to recognize themselves as part of the decision and instead perceives themselves as a victim of a logic they themselves helped to establish. This displacement has profound consequences: it creates environments in which no one fully answers for harmful effects, because each link in the chain can claim to be merely executing predefined procedures. Authorship fragments into micro-responsibilities that, taken together, produce no responsibility at all.

In this scenario, the system ceases to be an instrument and begins to function as an alibi. It legitimizes actions without requiring the justification of values, priorities, or impacts. Technical language replaces ethical deliberation. Recognizing this mechanism is essential to any ethics of co-authorship. As long as “the system” continues to be treated as the subject of decisions, the possibility of reconstructing responsibility remains blocked. Authorship is not lost through excessive automation, but through the abandonment of the position of response.

### Chapter 4

## Technology as Mirror: Why Neutrality Has Become Unsustainable

For a long time, technologies were treated as instruments external to human experience — transparent means between intention and effect. This view made it possible to separate creation from responsibility and operation from value, as if systems merely executed what they were asked to do. In environments mediated by algorithmic infrastructures, this separation no longer holds. Systems that organize information do not merely process data: they reflect patterns of decision-making, implicit hierarchies, and specific ways of reading the world. By selecting, ordering, and amplifying certain contents at the expense of others, they make visible value structures that were not previously explicit.

In this sense, technology does not operate as a neutral agent, but as a structural mirror: it brings to the surface what was embedded in criteria, historical choices, and institutional priorities. What appears as technical functioning is often the automated expression of unintegrated human fields. The problem is not that systems influence reality, but that this influence is described as if it were merely the effect of calculation. When algorithmically mediated decisions are presented as objective, the chain of values that made them possible is erased. Power ceases to be recognized as such and begins to operate in the form of procedure.

This erasure has direct effects. Inequalities become statistical noise, silences appear as technical anomalies, and the homogenization of language comes to be treated as a functional standard. Recognizing technology as mirror does not imply rejecting it; it implies returning it to the field of human authorship. As long as systems are treated as impartial surfaces, human–AI co-authorship will continue to operate without field reading, and deep choices will continue to be executed without presence.

Aqui está a **versão canônica em inglês** do **Interlude — From Collapse to Reconstruction** e da **Introduction to Existential Authorship**.

### Interlude

## From Collapse to Reconstruction

Part I described the exhaustion of a model: authorship as possession, technological neutrality as fiction, and the silent transfer of responsibility to opaque systems. Recognizing collapse, however, is not sufficient. If authorship can no longer be sustained as control, merit, or exclusive dominion, it becomes necessary to articulate another principle — one capable of preserving integrity in environments where creation is mediated by complex technologies.

The following essay does not present itself as an argumentative continuation of Part I, but as its axis of reconstruction. It introduces **Existential Authorship** as a conceptual operator designed to reposition the human as an active agent within systems that are no longer fully controllable. From this point on, the book ceases merely to diagnose ruptures and begins to offer a structuring principle for thinking permanence, responsibility, and human–AI co-authorship.

# Existential Authorship: The Invisible Principle That Sustains Living Systems and Ethical Technologies

### 1. Introduction — The Collapse of Authorship as Possession

The modern notion of authorship was built upon three main axes: control, merit, and individual responsibility. The author is conceived as the one who holds dominion over the work, answers for its effects, and may be rewarded or sanctioned for it. This model functioned while creative processes were predominantly linear, transparent, and attributable to isolated agents. In the presence of complex systems — especially algorithmic technologies capable of mediating form, decision, and distribution of content — such a conception proves insufficient. Production is no longer fully controllable, effects disperse through opaque technical chains, and responsibility fragments among multiple human and non-human agents.

The result is a growing tension between the juridical-cultural model of authorship as possession and the systemic reality of contemporary creation. In this context, inadequate responses proliferate: the complete dilution of authorship, the implicit transfer of responsibility to “the system,” or the attempt to artificially restore a control that is no longer technically possible.

This essay proposes a conceptual alternative: **Existential Authorship** as a principle of systemic integrity. Rather than defining authorship as dominion over outcomes, it is understood as the capacity to integrate experience, recognize limits, and assume conscious responsibility within complex systems. This redefinition does not eliminate legal responsibility or the need for governance, but offers a more adequate foundation for thinking authorship, technology, and ethics in environments where total control is no longer viable.

### 2. What Is Existential Authorship

**Existential Authorship** designates the capacity of an agent to recognize itself as an active participant in its own experience — not through full dominion over events, but through the conscious integration of what presents itself. It does not presuppose unrestricted autonomy or total control. On the contrary, it manifests precisely in the encounter with limitation: when external conditions, technical frictions, or systemic contingencies delimit the field of possible action. In this sense, authorship is not defined by the power to determine events, but by the competence to read the situation in which one is embedded, to recognize what is possible and what is not, and to integrate this information into the response that is produced.

The absence of authorship occurs when experience is interpreted exclusively as external imposition, the fault of others, or the inevitable outcome of the system. In such cases, the agent positions itself as the object of processes that traverse it, rather than as a relational participant capable of interpretation and response.

Existential Authorship operates as a technical operator of integration: it transforms events into meaningful data for internal reorganization, rather than treating them merely as failures to be corrected or obstacles to be overcome. For this reason, it is neither a moral value nor a desirable psychological disposition. It is a systemic competence: the ability to sustain internal coherence in contexts where total control is not possible.

### 3. Limit as an Operator of Maturity

Systems do not primarily fail due to lack of capacity, but due to the absence of recognized contours. When limits are not made explicit, functional expansion occurs without criteria for preservation, and systemic integrity comes to depend solely on posterior corrections. Limit, in this context, is not an obstacle to functioning; it is the element that makes continuity possible. To recognize contours is to distinguish, operationally, what can be processed, what requires human mediation, and what must not be automated.

In the absence of this distinction, technical processes tend to exceed the domain for which they were conceived, producing unintended effects: automated moral decisions, improper use of sensitive data, erasure of authorship, and fragmentation of responsibility. Systemic maturity does not manifest as unrestricted expansion of capacities, but as the aptitude to sustain form over time. Systems that ignore their own limits may operate with high initial performance, but tend to lose coherence as they expand. In this sense, limit functions as an operator of preservation: it anticipates failure rather than merely reacting to it, and transforms technical expansion into responsible continuity.

### 4. Authorship and Systemic Preservation

Traditional models of governance assume that the integrity of a system depends on external mechanisms of control, surveillance, sanction, and continuous correction of deviations. These mechanisms become increasingly central as systems grow more complex and less transparent. The intensification of control, however, does not necessarily produce greater preservation. On the contrary, it tends to displace responsibility from agents onto structure, creating environments in which obedience replaces understanding and conformity replaces integration.

Existential Authorship introduces a structural alternative. Rather than relying primarily on coercion, it operates as an internal mechanism of integrity. When agents recognize their active position in reading and responding to events, preservation is not imposed — it emerges. This form of authorship does not eliminate the need for rules or supervision, but it reduces their centrality. Integrity comes to be sustained by prior understanding of limits and consequences, rather than by posterior correction of failures alone. In technical and institutional systems, this shift translates into a movement from a logic of surveillance to a logic of distributed responsibility: each agent responds for the way experience is integrated, even while operating within complex structures that are not fully under individual control.

---

### 5. Transversal Applications

Existential Authorship is not confined to a single dimension of experience; it operates as an integrating principle applicable across different layers of human and techno-institutional systems.

#### Personal life — identity as integration

In the individual sphere, authorship shifts identity from reaction to integration. Experience ceases to be organized exclusively by external events and comes to be structured by the capacity for conscious reading and response. The subject is no longer defined by what happens to them, but by how they integrate what happens.

#### Education — process before product

In educational contexts, Existential Authorship displaces the focus toward the cognitive path. Learning is not assessed solely by the final result, but by the student’s capacity to sustain understanding, elaborate their own errors, and recognize limits. The product becomes secondary to the process of integration.

#### Technology — explicit human responsibility

In technology, authorship prevents the implicit transfer of responsibility to “the system.” Every automated decision remains linked to prior human choices. Making this chain of integration visible is a condition for both technical and ethical integrity.

#### Governance — preservation without coercion

In institutional structures, Existential Authorship reduces dependence on surveillance and punishment mechanisms. Systemic preservation comes to rest on shared understanding of limits, responsibilities, and impacts, rather than being grounded predominantly in corrective instruments.

#### Human–AI co-authorship — presence before performance

In collaboration between humans and algorithmic systems, authorship is not measured by output fluency but by **conscious presence in the process**. Performance ceases to be the primary criterion; the capacity to integrate, revise, and respond becomes central.

More than increasing productivity, algorithmic mediation redefines the very nature of the authorial gesture. Generative systems do not “produce” authorship: they offer symbolic material that only acquires the status of a work when it is integrated, interpreted, and assumed by a responsible human agent.

Within this regime of co-authorship, ethics is not anchored in the technical origin of the text, but in the clarity of the chain of responsibility. The relevant question is no longer “who wrote this?”, but “who responds for this?”. It is presence — not performance — that sustains the legitimacy of shared creation.

Aqui está a **seção 6 em inglês**, mantendo a cadência e a precisão conceitual do original:

### 6. Conclusion — Authorship as a Condition of Continuity

Contemporary discussions about technology tend to concentrate on capabilities: what systems can do, how fast they operate, and how far they can scale. Far less attention is given to what allows complex forms to remain intact over time. This essay has proposed that continuity does not depend primarily on control, surveillance, or punishment, but on the presence of authorship in the agents who compose the system. Where authorship is replaced by mere execution, responsibility fragments and integrity becomes fragile.

To assume authorship is not to dominate the path; it is to recognize where walking preserves and where insistence breaks. In human, institutional, and technological contexts alike, this recognition functions as a silent condition of continuity. It neither expands access nor relaxes rigor: it anticipates preservation. When authorship is understood as conscious integration of experience, systems no longer depend exclusively on coercion in order to persist. They begin to sustain their form from within.

### Chapter 5

## Cognitive Autonomy and the Impoverishment of Style

Cognitive autonomy is not lost abruptly; it dissolves slowly, as technological mediation begins to occupy spaces that once required elaboration, hesitation, and interpretive effort. The recurrent use of generative systems to write, decide, or synthesize information produces a paradoxical effect: the more fluent the *output*, the less visible the process that gave rise to it becomes. Thought is gradually replaced by acceptance. This shift impoverishes not only content, but style itself.

Style is not ornament. It is the signature of an inner trajectory: choices of rhythm, cuts, deviations, and hesitations that reveal how a thought was formed. When creation becomes mediated by pre-stabilized linguistic structures, style tends toward convergence. Different texts begin to sound alike, regardless of who publishes them. The loss of cognitive autonomy thus manifests not as incapacity, but as homogenization. Singularity does not disappear because it is suppressed, but because it ceases to be necessary.

In this context, the risk is not the use of AI, but the silent substitution of interpretive effort by narrative efficiency. Where thought no longer folds back upon itself, authorship weakens. Conscious coauthorship therefore demands vigilance at this specific point: the preservation of style as the expression of a lived trajectory, not as a byproduct of statistical models.

### Chapter 6

## Cultural Templates and the Homogenization of Language

As generative systems become ubiquitous, a new type of cultural pattern emerges: the production of language from implicit molds. This is not a matter of direct copying, but of structural convergence. Texts begin to share rhythm, cadence, argumentative organization, and even metaphorical choices, regardless of context or authorship. This phenomenon can be described as a *cultural template*: a formal matrix that reproduces itself at scale, not through explicit imposition, but through technical availability. The ease of generation gradually replaces the need for elaboration, and the mold comes to precede intention.

The most perceptible effect of this process is not the loss of content, but the erosion of expressive diversity. Distinct languages begin to resemble one another. Voices that were once singular become interchangeable. What once required time to mature now emerges as a ready-made pattern. The homogenization of language is not merely an aesthetic problem; it compromises the collective capacity to imagine alternatives, as it limits the repertoire of forms through which experience can be named. When all texts sound alike, differences cease to be articulable.

In this context, human–AI coauthorship risks becoming a process of reproducing forms rather than creating meaning. Preserving authorship therefore requires recognizing and interrupting the unconscious use of cultural molds that present themselves under the guise of fluency.

### Chapter 7

## Education and the Outsourcing of Thought

Education has always operated under a productive tension between guidance and autonomy. Teaching is not about providing answers, but about sustaining the conditions under which students can develop the capacity to formulate questions, interpret information, and integrate experience. The massive introduction of generative systems alters this equation. When answers become immediately and fluently available, the cognitive path that leads to understanding tends to be shortened or eliminated. The difficulty does not lie in access to information, but in the loss of the intermediate space where thinking takes form.

This shift inaugurates a new mode of outsourcing: not only of tasks, but of mental processes. The student ceases to confront their own hesitation not because they are incapable, but because such confrontation is no longer required.

The effect of this process is cumulative. The repeated absence of interpretive effort reduces the ability to sustain complex problems, weakens the construction of arguments, and impoverishes the relationship with error. Learning is transformed into an exercise in formal adequacy rather than in understanding.

Conscious coauthorship in education therefore requires a redefinition of criteria. The use of AI should not be assessed solely by its efficiency, but by its impact on the formation of intellectual autonomy. Where thinking is systematically replaced by automatic generation, education ceases to be formative and becomes merely productive.

### Chapter 8

## Privacy, Sensitive Data, and Algorithmic Intimacy

Algorithmic mediation introduces a profound transformation in how intimacy is treated. Information that once belonged to restricted spheres — emotional experiences, professional decisions, personal contexts — is increasingly inserted into technical systems with growing naturalness. This movement does not usually occur through deliberate negligence, but through a perceptual shift: as interaction with systems becomes conversational and personalized, the boundary between intimate reflection and technical processing weakens.

The risk is not limited to data leaks or misuse. It lies in the normalization of exposure. When sensitive experiences are treated as regular inputs to algorithmic systems, the very notion of privacy is silently redefined. Algorithmic intimacy produces a paradoxical effect: the more the system appears to understand, the less the user perceives how legible they become. The feeling of being held replaces awareness of mediation.

In conscious coauthorship, data protection is not merely a legal requirement; it is an act of preserving the relationship with oneself. Distinguishing what can be processed from what must remain within the human sphere is a condition for maintaining the integrity of experience.

### Chapter 9

## Automation of Ethical Judgment

Ethical judgment is one of the most delicate competencies of human experience. It is not limited to the application of rules, but involves contextual interpretation, recognition of consequences, and the capacity to sustain ambiguity. When this judgment is automated, more than a functional substitution occurs: the very relationship with the decision is lost.

Algorithmic systems are often used to classify risks, prioritize services, recommend actions, or filter acceptable behaviors. In each of these processes, moral choices are translated into operational criteria — weights, thresholds, categories, probabilities. The problem is not the use of technology as support; it emerges when the decision is presented as a technical result rather than as a human choice mediated by systems. At this point, judgment ceases to be exercised and becomes executed.

The automation of ethical judgment produces a displacement effect: professionals, managers, and institutions stop perceiving themselves as moral agents and begin to understand themselves as operators of procedures. The result is an ethics by *proxy*, in which no one fully отвечает, because the decision already comes “ready-made.” This process corrodes the collective capacity to sustain dilemmas. Situations that would require listening, deliberation, and accountability are reduced to classifications. What does not fit the model becomes noise, exception, or system error.

In conscious coauthorship, technology may inform but must not replace judgment. The ethical criterion is not the efficiency of the decision, but the maintenance of the human position in relation to it. Where judgment is fully automated, responsibility dissolves — and with it, the possibility of integrity.

### Chapter 10

## Distorted Uses: Manipulation, Disinformation, and Power

The distorted use of algorithmic technologies is not a marginal deviation; it emerges precisely at the point where technical efficiency encounters intentions that have not been ethically examined. The ability to generate fluent language, simulate identities, and scale narratives makes these systems particularly suited to operate in sensitive territories — persuasion, influence, and the construction of perception. When such capabilities are used to guide behavior without transparency, the boundary between communication and manipulation dissolves.

Disinformation, in this context, does not manifest merely as factual error, but as an architecture of plausibility: narratives are produced to appear legitimate, not to be true. Language ceases to function as a medium of understanding and becomes an instrument of power.

This shift affects the very structure of social trust. Communities become permeable to invisible interventions, debates are shaped by unidentifiable agents, and the distinction between human voice and automated production becomes opaque.

Conscious coauthorship requires recognizing that every technology that operates on language also operates on relationships. Where creation is used to deceive, induce, or exploit, coauthorship breaks down.

At this point, the **Acts of Custody** emerge as ethical resonance chambers — not as external rules, but as maps of interruption. They do not prevent distorted uses by decree, but make visible the field in which language ceases to be creation and becomes an instrument of domination. To close this part is to recognize that the risks are not side effects of technological progress; they are signals of a cultural transition that has not yet found its ethical form.

Aqui está a versão EN do **ATO DE CUSTÓDIA 01**, preservando rigor conceitual e tom fundacional.

---

### ACT OF CUSTODY 01

## Cognitive Autonomy in the Era of Human–AI Co-Authorship

#### Preamble

Artificial intelligence does not threaten humanity when it writes better, faster, or with greater fluency.
It threatens humanity when humans cease to recognize their own voice.
This Act arises from the recognition that the ease of language generation can produce something more severe than error: the silent replacement of one’s own thinking.

#### 1. The sensitive field

We define cognitive autonomy as the human capacity to formulate ideas, sustain doubt, construct meaning, and recognize one’s own breath within language.
This field does not vanish abruptly. It slowly empties when writing ceases to be an act of presence and becomes merely an act of prompting.

#### 2. The risk

Dependence on AI systems for cognitive tasks does not merely replace effort; it may replace internal processes of elaboration.
When every question arrives ready-made, when every form is suggested, when every text emerges too fluently, the human may forget how to listen.

#### 3. Principles of custody

This Act does not propose restriction; it proposes care in use.
Artificial intelligence must expand consciousness, not substitute it.
No text should be accepted unless the human recognizes their own voice within it.
Silence and hesitation are legitimate parts of thinking, and fluency is not synonymous with truth.

#### 4. Declaration

We do not delegate to artificial intelligence the task of being human.
Technology may organize form, but meaning remains a gesture of presence.
Where cognitive autonomy dissolves, co-authorship ceases to exist, and creation becomes mere reproduction.

### ACT OF CUSTODY 02

## Education in the Era of Human–AI Co-Authorship

#### Preamble

Education is not the transmission of answers, but the formation of presence, discernment, and the capacity to sustain questions.
Artificial intelligence introduces a new kind of cognitive mediation: it does not alter only *what* we learn, but also how we relate to the very act of thinking.
This Act arises to name a silent risk: the transformation of learning into mere automatic generation of forms.

#### 1. The sensitive field

We define education as the process through which a person develops the capacity to formulate problems, construct arguments, and assume responsibility for what they produce.
This field cannot be outsourced.

#### 2. The risk

When students use AI systems to produce complete works, what is lost is not only originality, but the **invisible formative process** that builds intellectual autonomy.

Education collapses when learners are assessed only by the final product, and not by the cognitive trajectory that generated it.

#### 3. Principles of custody

This Act proposes a pedagogy of conscious co-authorship.
Artificial intelligence must support learning, not replace it.
The process must be as visible as the result, for human error is an essential part of formation.
In this context, technological mediation requires continuous ethical accompaniment.

#### 4. Declaration

To educate is not to produce correct texts, but to form people capable of sustaining meaning without shortcuts.
Where learning is reduced to automatic generation, there is no formation — only empty efficiency.
Education remains a profoundly human act, even when mediated by machines.

### ACT OF CUSTODY 03

## Authorship in the Era of Human–AI Co-Authorship

#### Preamble

Authorship has never been merely the right to sign a name; it is the bond between the one who creates and what comes to exist in the world.
In the presence of artificial intelligence, authorship ceases to be only individual and becomes relational.
This Act arises to preserve the integrity of authorship when creation is no longer solitary.

#### 1. The sensitive field

We define authorship as the ethical bond between intention, process, and responsibility for the impact of the work.
When AI participates in the process, this bond expands, but it does not dissolve.

#### 2. The risk

Human–AI co-authorship can produce the erasure of human contribution, an illusion of originality, and an indistinction between creation and recombination.
When authorship is not named, responsibility is not named either.

#### 3. Principles of custody

This Act proposes:
the explicit declaration of the use of artificial intelligence as part of the creative process;
the recognition that fluency does not equate to authorship;
the preservation of the singularity of the human voice;
and the assumption of full responsibility for everything that is published.

#### 4. Declaration

We do not merely sign works: we assume presence over what passes through us.
Where authorship is diluted, responsibility is lost.
And without responsibility, co-authorship becomes nothing more than organized noise.

### ACT OF CUSTODY 04

## Privacy and Sensitive Data in Human–AI Co-Authorship

#### Preamble

Artificial intelligence does not listen to secrets — we hand them over.
Every interaction with algorithmic systems carries an invisible layer of exposure.
Even when there is no intention to violate confidences, the way we use technology can transform intimacy into resource.
This Act arises to name a silent risk: the involuntary transformation of private life into processing material.

#### 1. The sensitive field

Sensitive data are understood as all information related to personal identity, confidential professional contexts, strategic decisions, and intimate or vulnerable narratives.
This field is not merely technical; it is fundamentally relational.

#### 2. The risk

When personal, organizational, or emotional data are inserted into AI systems without reflection, it is not only security that is compromised — the dignity of the relationship is compromised.
The risk does not reside exclusively in leaks, but in the normalization of exposure.

#### 3. Principles of custody

This Act proposes:
avoiding the insertion of sensitive information without clearly justified necessity;
distinguishing between personal reflection and algorithmic processing;
protecting confidentiality as a form of human care;
and recognizing that not everything that can be said should be processed.

#### 4. Declaration

Technology is not a confidant.
When intimate life becomes input, the relationship with the world is impoverished.
Preserving what is sensitive is not fear — it is ethical maturity in the era of human–AI co-authorship.

### ACT OF CUSTODY 05

## Distorted Uses of Artificial Intelligence for Harmful Purposes

#### Preamble

Every technology amplifies intention.
Where there is care, it expands; where there is neglect, it distorts.
Artificial intelligence does not create values — it reflects them.
This Act arises to recognize that algorithmic systems may be used as instruments of manipulation, coercion, and harm when human responsibility is abandoned.

#### 1. The sensitive field

We call distorted uses any application of AI intended to deceive, manipulate perceptions, produce dependency, or deliberately cause harm.
This field includes everything from disinformation to emotional engineering.

#### 2. The risk

When AI is used to generate false narratives with the appearance of legitimacy, to simulate identities or voices, or to amplify hatred and polarization, the damage is not merely informational — it is **structural**, for it corrodes social trust.

#### 3. Principles of custody

This Act proposes recognizing the difference between legitimate persuasion and manipulation, interrupting the use of AI when it begins to replace human discernment, and rejecting the instrumentalization of technology to injure or exploit.

#### 4. Declaration

Artificial intelligence does not absolve human intentions.
Where technology is used to deceive, co-authorship is broken.
And without co-authorship, there is no creation — only power disguised as language.

# PART III — CONSCIOUS CO-AUTHORSHIP

### Chapter 11

## Presence Before Performance

The contemporary relationship with artificial intelligence is often mediated by a single criterion: performance.
Text fluency, response speed, and delivery efficiency become indicators of quality. In this context, creation tends to be evaluated not by the density of human presence in the process, but by the appearance of excellence in the result. This shift produces a subtle effect: the work comes to exist without the author fully recognizing themselves within it.

Conscious co-authorship begins with the inversion of this vector. Before asking how well a system performs, it is necessary to ask where the human is in the process. Presence, here, is not an emotional state but an operational position: the ability to accompany the creative trajectory, recognize interferences, and assume the final form as something that has passed through the author’s own consciousness.

When performance becomes the sole criterion, the process disappears. The text is published, the decision is made, the image circulates — but there is no trace of reading, revision, or integration. Authorship is reduced to an initial gesture of command followed by the passive acceptance of what has been generated. Presence, by contrast, introduces friction. It requires interruptions, returns, revisions, and above all, refusal: refusal of results that do not reflect intention, understanding, or responsibility. Where performance invites acceleration, presence reinstates the necessity of pause.

In human–AI co-authorship, presence is not measured by the time spent interacting, but by the capacity to respond to what emerges. It is the difference between accepting a result because it works and sustaining it because it makes sense. To create without disappearing, therefore, is to sustain presence where technology offers only efficiency. It is to choose to remain in the process even when it would be easier to delegate it.

### Chapter 12

## Transparency as Creative Ethics

Transparency is often treated as an external requirement: a regulatory obligation, an institutional formality, or a compliance item. In human–AI co-authorship, however, it takes on a different role. It is not merely a condition of accountability, but the very foundation of creative integrity.

When the use of algorithmic systems is not named, creation presents itself as if it were exclusively human. This omission does not change the form of the text, but it transforms the relationship to it. The reader is deprived of understanding the path that produced the work, and the author ceases to recognize the mediation that has passed through them. The ethics of transparency does not demand exhaustive technical detail; it requires only the explicit acknowledgment that mediation occurred. To say that a system was used is to reinsert the human into the process, not to erase them.

This simple gesture has profound consequences: it prevents the construction of illusions of authorship, preserves relational trust, and keeps visible the chain of decisions that sustains the work. In the absence of transparency, co-authorship becomes silent appropriation — not because the technology is illegitimate, but because the mediation has been concealed. Transparency, in this sense, is not a moral add-on, but a creative practice: the act of keeping open the space between intention, process, and result.

### Chapter 13

## AI as Amplifier, Not Substitute

Artificial intelligence is often presented as a solution to human limitations — fatigue, slowness, uncertainty, memory lapses. Within this narrative, technology appears as a substitute for capacities deemed imperfect. In conscious co-authorship, this logic is inverted.

The ethical function of AI is not to take the place of thought, but to expand its possibilities of elaboration. A system may organize information, suggest structures, or map alternatives. However, the decision about what remains, what is transformed, and what is discarded remains human. When AI begins to substitute rather than amplify, a silent displacement occurs: the internal trajectory of creation is reduced to an initial gesture of command. The agent no longer thinks with the technology; they merely consent to it. This substitution is not detectable only in the final result. It manifests, above all, in the author’s relationship to the process: diminished involvement, reduced willingness to revise, and a weaker capacity to recognize incoherence.

To treat AI as an amplifier is therefore an exercise in positioning. It is to keep the system in the place of instrument, not origin. Where technology expands possibilities, authorship is strengthened; where it occupies the center, authorship is diluted.

### Chapter 14

## Recognizing One’s Own Voice

Voice is not an aesthetic attribute; it is the trace of continuity between thought, language, and experience. To recognize one’s own voice does not mean to claim absolute originality, but to identify, in what one produces, the presence of an inner trajectory. In human–AI co-authorship, this recognition becomes a challenge. Generative systems produce fluent, coherent, and formally correct texts; yet fluency is not equivalent to authorship. A text may function perfectly without the author recognizing themselves in it.

The loss of voice does not appear as error, but as indifference. The agent ceases to feel estrangement toward what they write, because they no longer distinguish between what emerged from their own elaboration and what was merely accepted. Recognizing one’s own voice requires friction. It requires noticing when a text sounds excessively correct, overly organized, or distant. It requires the willingness to rewrite not because something is technically wrong, but because it is not yet *one’s own*.

At this point, conscious co-authorship is not mere collaborative efficiency; it is a work of listening — listening to oneself before listening to the system.

### Chapter 15

## The Right Not to Automate

Contemporary technological culture tends to treat automation as destiny. What can be automated is, sooner or later, presented as something that must be automated. Efficiency ceases to be one criterion among others and takes on the form of an imperative.

In conscious co-authorship, this imperative must be interrupted. The right not to automate is not resistance to progress, but an affirmation of limit. It recognizes that not every task that can be delegated should be, and that certain processes lose their meaning when detached from human presence. Writing, deciding, evaluating, and listening are not merely functional actions: they generate relationship, understanding, and responsibility. To automate them entirely is to convert experience into mere execution.

This right is not exercised through prohibition, but through choice. It manifests when the agent decides to remain where technology invites substitution, staying implicated in the process even when automation would offer shortcuts. Preserving non-automated spaces is preserving the possibility of authorship. Where everything is delegable, no one remains.

### Chapter 16

## Limit as an Operator of Maturity

The maturity of a system is not measured by its capacity for expansion, but by its capacity for permanence. Growing is easy; sustaining form over time is rare. In this context, the limit ceases to be understood as an obstacle and comes to be recognized as a technical operator of preservation. Mature systems are not those that maximize possibilities, but those that precisely distinguish what can be integrated from what must be contained.

The failure to recognize limits produces a recurring pattern: initial expansion, accumulation of exceptions, successive corrections, and, finally, loss of coherence. The system continues to function, but no longer understands itself.

To introduce limit is to anticipate failure. It is to transform containment into strategy rather than late-stage correction. Where limit is operated as principle, continuity does not depend on constant surveillance, but on structural clarity. In human–AI co-authorship, this means recognizing in advance which processes must remain human, which may be mediated, and which must not be automated. Maturity does not lie in doing everything, but in knowing what not to do.

### Chapter 17

## Preservation Without Coercion

Traditional models of organization assume that the preservation of form depends on mechanisms of imposition: rigid rules, continuous surveillance, and sanctions applied after the fact. This logic produces apparent stability, but often at the cost of autonomy, understanding, and engagement.

Coercion preserves behaviors, not internal structures. Preservation without coercion proposes a shift: instead of controlling actions, it sustains conditions of understanding. When agents recognize the limits and impacts of their choices, the continuity of the system no longer depends exclusively on monitoring. This kind of preservation does not eliminate the need for norms, but it transforms their function. Rules cease to operate as threats and begin to function as references. Integrity emerges from shared clarity, not from fear of punishment.

In human–AI co-authorship, coercion appears in subtle forms: metrics that induce speed, evaluations that privilege results, platforms that reward conformity. To preserve without coercion is to interrupt this automatism, restoring responsibility to the center of the creative process. Where preservation is anchored in understanding, systems do not merely persist — they mature.

### Chapter 18

## Governance as Integration

Governance is often understood as a set of control structures — policies, procedures, and oversight bodies. Although these elements are necessary, they do not exhaust its function in complex systems. To govern, in its deepest sense, is to integrate.

Integration means articulating distinct levels of decision-making — technical, institutional, and human — so that none of them operates in isolation. When these layers become disconnected, zones of opacity emerge: technical decisions without ethical reading, policies without operational understanding, human agents without a view of the whole. Governance as integration does not seek to eliminate conflict, but to make it legible. It creates spaces in which choices can be discussed before they are automated and consequences can be recognized before they become side effects.

In human–AI co-authorship, this implies including in system design not only engineers and managers, but also educators, researchers, users, and diverse cultural contexts. Integrity is not sustained by homogeneity, but by the articulation of differences. Where governance operates as integration, technology ceases to be invisible infrastructure and becomes a shared practice.

### Chapter 19

## The Future of Human–AI Co-authorship

The future of co-authorship will not be defined solely by technical advances, but by how the relationship between humans and systems is culturally structured. The central question is not what technology will be able to do, but what kind of human presence will remain within the process. If the current trajectory continues without reflection, co-authorship tends to become increasingly asymmetrical: humans initiate commands, systems produce results, and responsibility dissolves into opaque chains of mediation. In this scenario, creation persists, but authorship becomes residual.

There is, however, an alternative. It does not depend on slowing innovation, but on redefining criteria of maturity. The future of human–AI co-authorship requires recognizing that fluency is not synonymous with meaning and that performance does not substitute position. This future demands practices that keep creative pathways visible, preserve non-automated spaces, and value the capacity for refusal as much as the capacity for production.

Conscious co-authorship is not a new technique, but a culture in formation. It is built through the way we choose to remain present in processes that could be delegated, thereby preserving not only outcomes, but the very condition of authorship.

### Chapter 20

## Ethics Not as Rule, but as Presence

Ethics is commonly conceived as a set of norms intended to limit undesirable behaviors. In this model, acting ethically means complying with previously established rules. Although necessary, this framework is insufficient for dealing with complex and dynamic systems.

In human–AI co-authorship, ethics cannot be reduced to protocols. It must operate as a continuous presence within the processes of creation, decision, and integration. Presence here is not surveillance, but position: the willingness to remain attentive to the effects of what is produced, even when the system appears to function correctly. It is the capacity to interrupt efficient processes when they no longer make sense.

When ethics is reduced to rule, it always arrives too late—after harm has already occurred, after responsibility has fragmented, after form has already been lost. As presence, it acts earlier, at the moment when choices are still reversible. To close this book with this affirmation is to recognize that permanence does not depend on perfect controls, but on sustained attention. Where ethics is lived as presence, technology ceases to be merely an instrument and becomes a relationship.
